# Attention-Is-All-You-Need-Paper-Replicating
This project is a from-scratch replication of the “Attention Is All You Need” paper (Vaswani et al., 2017), which introduced the Transformer architecture — the foundation of modern large language models such as GPT and BERT.
