# ğŸ§  Transformer From Scratch â€” *Replicating â€œAttention Is All You Needâ€*

Reimplementation of the **Transformer architecture** proposed in  
ğŸ“„ [Vaswani et al., 2017 â€” *Attention Is All You Need*](https://arxiv.org/abs/1706.03762)

This project faithfully reproduces the model described in the paper, built entirely **from scratch using PyTorch**.  
Every component â€” from **positional encoding** to **multi-head attention**, **feed-forward layers**, and the **Noam learning rate scheduler** â€” follows the original architecture and equations.

---

## ğŸ“¸ Model Overview

### Figure 1: Transformer Architecture  
![Figure 1: The Transformer - model architecture](images/Figure%201:%20The%20Transformer%20-%20model%20architecture.png)  
*Overall encoderâ€“decoder architecture consisting of stacked attention and feed-forward layers.*

---

### Figure 2: Scaled Dot-Product & Multi-Head Attention  
![Figure 2: Scaled Dot-Product & Multi-Head Attention](images/Figure%202:%20(left)%20Scaled%20Dot-Product%20Attention.%20(right)%20Multi-Head%20Attention%20consists%20of%20several%20attention%20layers%20running%20in%20parallel.png)  
*The left side shows how attention weights are computed using scaled dot-products of queries and keys.  
The right side demonstrates how multiple attention heads work in parallel to capture different dependencies.*

---

### Figure 3: Example of Attention Visualization  
![Figure 3: Example of the attention mechanism](images/Figure%203:%20An%20example%20of%20the%20attention%20mechanism%20following%20long-distance%20dependencies%20in%20the%20encoder%20self-attention%20in%20layer%205%20of%206.%20Many%20of%20the%20attention%20heads%20attend%20to%20a%20distant%20dependency%20of%20the%20verb%20â€˜makingâ€™,%20completing%20the%20phrase%20â€˜making...more%20difficultâ€™.%20Attentions%20here%20shown%20only%20for%20the%20word%20â€˜makingâ€™.%20Different%20colors%20represent%20different%20heads.%20Best%20viewed%20in%20color.png)  
*Visualization from the paper: attention heads in layer 5 focusing on distant relationships like â€œmakingâ€ â†’ â€œdifficultâ€.  
Different colors represent different heads.*

---
## ğŸ§© Project Structure
```bash

Attention-Is-All-You-Need-Paper-Replicating/
â”‚
â”œâ”€â”€ ğŸ“ src/
â”‚ â”œâ”€â”€ 1_input_embedding/
â”‚ â”‚ â”œâ”€â”€ embeddings.py â†’ TokenEmbedding (makale 3.4)
â”‚ â”‚ â””â”€â”€ positional_encoding.py â†’ Sinusoidal encoding (makale 3.5)
â”‚ â”‚
â”‚ â”œâ”€â”€ 2_attention/
â”‚ â”‚ â”œâ”€â”€ scaled_dot_product.py â†’ softmax(QKáµ€ / âˆšdâ‚–)V (makale 3.2.1)
â”‚ â”‚ â””â”€â”€multi_head_attention.py â†’ Concat(headâ‚,â€¦,headâ‚•)Wâ‚€ (makale 3.2.2)
â”‚ â”‚ 
â”‚ â”œâ”€â”€ 3_feed_forward/
â”‚ â”‚ â””â”€â”€ positionwise_ffn.py â†’ FFN(x)=max(0,xWâ‚+bâ‚)Wâ‚‚+bâ‚‚ (makale 3.3)
â”‚ â”‚
â”‚ â”œâ”€â”€ 4_encoder_decoder/
â”‚ â”‚ â”œâ”€â”€ encoder_layer.py â†’ MultiHead + FFN + Residual (makale 3.1)
â”‚ â”‚ â”œâ”€â”€ decoder_layer.py â†’ Masked + Encoder-Attention + FFN
â”‚ â”‚ â”œâ”€â”€ encoder.py â†’ 6-layer Encoder stack
â”‚ â”‚ â””â”€â”€ decoder.py â†’ 6-layer Decoder stack
â”‚ â”‚
â”‚ â”œâ”€â”€ 5_transformer/
â”‚ â”‚ â””â”€â”€ transformer.py â†’ Encoder + Decoder + Linear + Softmax (makale 3.1 genel mimari)
â”‚ â”‚
â”‚ â”œâ”€â”€ 6_training/
â”‚ â”‚ â”œâ”€â”€ optimizer.py â†’ Noam LR schedule (makale 5.2)
â”‚ â”‚ â”œâ”€â”€ loss_fn.py â†’ CrossEntropy + Label Smoothing (makale 5.3)
â”‚ â”‚ â”œâ”€â”€ train_utils.py â†’ train_step(), create_masks()
â”‚ â”‚ â””â”€â”€ regularization.py â†’ Dropout (makale 5.3)
â”‚
â”œâ”€â”€ ğŸ“images/
â”‚ â”œâ”€â”€ Figure 1: The Transformer - model architecture.png
â”‚ â”œâ”€â”€ Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel.png
â”‚ â””â”€â”€ Figure 3: An example of the attention mechanism following long-distance dependencies in the encoder self-attention in layer 5 of 6..png
â”‚
â”‚â””â”€â”€requirements.txt
```
---
## âš¡Feedback

For feedback or questions, contact: [barkin.adiguzel@gmail.com](mailto:barkin.adiguzel@gmail.com)




